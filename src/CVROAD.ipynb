{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2N_n3Ib0mej"
      },
      "source": [
        "# Загрузка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsHWJ2F30uh0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import gdown\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def merge_directories(dir1, dir2, output_dir):\n",
        "    \"\"\"\n",
        "    Объединяет содержимое поддиректорий из dir1 и dir2 в одну структуру в output_dir.\n",
        "    Используется один общий прогресс-бар для всех файлов.\n",
        "\n",
        "    :param dir1: str, путь к первой директории.\n",
        "    :param dir2: str, путь ко второй директории.\n",
        "    :param output_dir: str, путь к результирующей директории.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    subdirs = [d for d in os.listdir(dir1) if os.path.isdir(os.path.join(dir1, d))]\n",
        "\n",
        "    total_files = 0\n",
        "    for subdir in subdirs:\n",
        "        total_files += len(os.listdir(os.path.join(dir1, subdir)))\n",
        "        if os.path.exists(os.path.join(dir2, subdir)):\n",
        "            total_files += len(os.listdir(os.path.join(dir2, subdir)))\n",
        "\n",
        "    with tqdm(total=total_files, desc=\"Объединение файлов\") as pbar:\n",
        "        for subdir in subdirs:\n",
        "            dir1_subdir_path = os.path.join(dir1, subdir)\n",
        "            dir2_subdir_path = os.path.join(dir2, subdir)\n",
        "            output_subdir_path = os.path.join(output_dir, subdir)\n",
        "\n",
        "            if not os.path.exists(dir2_subdir_path):\n",
        "                print(f\"Поддиректория {subdir} отсутствует в {dir2}. Пропуск.\")\n",
        "                continue\n",
        "\n",
        "            if not os.path.exists(output_subdir_path):\n",
        "                os.makedirs(output_subdir_path)\n",
        "\n",
        "            for file_name in os.listdir(dir1_subdir_path):\n",
        "                src_file = os.path.join(dir1_subdir_path, file_name)\n",
        "                if os.path.isfile(src_file):\n",
        "                    shutil.copy(src_file, output_subdir_path)\n",
        "                    pbar.update(1)\n",
        "\n",
        "            for file_name in os.listdir(dir2_subdir_path):\n",
        "                src_file = os.path.join(dir2_subdir_path, file_name)\n",
        "                if os.path.isfile(src_file):\n",
        "                    shutil.copy(src_file, output_subdir_path)\n",
        "                    pbar.update(1)\n",
        "\n",
        "    print(f\"Объединение завершено. Результат сохранен в {output_dir}.\")\n",
        "\n",
        "\n",
        "url = \"https://fall.cv-gml.ru/task_file/15/additional_files\"\n",
        "\n",
        "headers = {\n",
        "    \"Cookie\": \"session=.eJwlzs1KBTEMQOF36dpFkuZnel9mSNMERVGY0ZXcd7fg9oMD57eddeX92h7lH3e-tPNttUfj5RC1hJU1M7APGUM3iodCmU9FDF-SLOQ6vc9pgIcw9-LOHANKBFkHroqQg0GqOKZhArGqrCQjWEyTUxAiJdC4wKIGtz3yc-f1f0PEY0vcV53fX-_5uc3Ra6ZBHgbeYbnKri2dRAgFyg-VgmzPP9v1QBE.Z4Tjug.mhYU5xb79OKI-ezR_Icya5Jght0\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(\"additional_files.zip\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Файл успешно скачан и сохранен как 'additional_files.zip'.\")\n",
        "else:\n",
        "    print(f\"Ошибка при скачивании. Код ответа: {response.status_code}\")\n",
        "    print(f\"Тело ответа:\\n{response.text}\")\n",
        "\n",
        "zip_file_path = \"additional_files.zip\"\n",
        "\n",
        "output_dir = \"/content/additional_files\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(f\"Файлы распакованы в {output_dir}\")\n",
        "\n",
        "file_id = \"1C1oQ_mXwPvUYKOjpvd0YTNQFSL6-OdZb\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "dataset_path = \"/content/synt.zip\"\n",
        "output_extract_path = \"/content/dataset/\"\n",
        "\n",
        "gdown.download(url, dataset_path, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_extract_path)\n",
        "\n",
        "print(f\"Датасет распакован в {output_extract_path}\")\n",
        "\n",
        "\n",
        "dir1 = \"/content/additional_files/cropped-train\"\n",
        "dir2 = \"/content/dataset/synt\"\n",
        "output_dir = \"/content/mixdataset\"\n",
        "\n",
        "merge_directories(dir1, dir2, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uDssHF02G_"
      },
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwqFX8Ne1JZX"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import typing\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import scipy\n",
        "import skimage\n",
        "import skimage.filters\n",
        "import skimage.io\n",
        "import skimage.transform\n",
        "import torch\n",
        "import torchvision\n",
        "import tqdm\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from albumentations import Compose, Resize, RandomCrop, HorizontalFlip, Normalize\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vAp3xC7e1Ndx"
      },
      "outputs": [],
      "source": [
        "class DatasetRTSD(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Класс для чтения и хранения датасета.\n",
        "\n",
        "    :param root_folders: список путей до папок с данными\n",
        "    :param path_to_classes_json: путь до classes.json\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_folders: typing.List[str],\n",
        "        path_to_classes_json: str,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        with open(path_to_classes_json, 'r') as file:\n",
        "            self.classes_info = json.load(file)\n",
        "        self.classes, self.class_to_idx = self.get_classes(path_to_classes_json)\n",
        "\n",
        "        self.samples = []\n",
        "        for folder in root_folders:\n",
        "            for class_name in os.listdir(folder):\n",
        "                class_folder = os.path.join(folder, class_name)\n",
        "                if os.path.isdir(class_folder):\n",
        "                    class_idx = self.class_to_idx.get(class_name, -1)\n",
        "                    for img_name in os.listdir(class_folder):\n",
        "                        img_path = os.path.join(class_folder, img_name)\n",
        "                        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            self.samples.append((img_path, class_idx))\n",
        "\n",
        "        self.classes_to_samples = {idx: [] for idx in self.class_to_idx.values()}\n",
        "        for idx, (_, class_idx) in enumerate(self.samples):\n",
        "            self.classes_to_samples[class_idx].append(idx)\n",
        "\n",
        "        self.transform = Compose([\n",
        "            Resize(256, 256),\n",
        "            RandomCrop(224, 224, p=0.5),\n",
        "            HorizontalFlip(p=0.5),\n",
        "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            Resize(224, 224),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index: int) -> typing.Tuple[torch.Tensor, str, int]:\n",
        "        \"\"\"\n",
        "        Возвращает тройку: тензор с картинкой, путь до файла, номер класса файла (если нет разметки, то \"-1\").\n",
        "        \"\"\"\n",
        "        img_path, class_idx = self.samples[index]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        transformed = self.transform(image=np.array(image))\n",
        "        image_tensor = transformed[\"image\"]\n",
        "\n",
        "        return image_tensor, img_path, class_idx\n",
        "\n",
        "    @staticmethod\n",
        "    def get_classes(\n",
        "        path_to_classes_json,\n",
        "    ) -> typing.Tuple[typing.List[str], typing.Mapping[str, int]]:\n",
        "        \"\"\"\n",
        "        Считывает из classes.json информацию о классах.\n",
        "\n",
        "        :param path_to_classes_json: путь до classes.json\n",
        "        \"\"\"\n",
        "        with open(path_to_classes_json, 'r') as file:\n",
        "            class_data = json.load(file)\n",
        "\n",
        "        class_to_idx = {class_name: idx for idx, (class_name, _) in enumerate(class_data.items())}\n",
        "\n",
        "        classes = [class_name for class_name in class_data.keys()]\n",
        "\n",
        "        return classes, class_to_idx\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает размер датасета (количество сэмплов).\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def get_rare_images(self) -> typing.List[str]:\n",
        "        \"\"\"\n",
        "        Функция для нахождения путей к изображениями с типом \"rare\" и подсчета их количества.\n",
        "        Возвращает список путей и количество изображений.\n",
        "        \"\"\"\n",
        "        rare_images = []\n",
        "        for img_path, class_idx in self.samples:\n",
        "            class_name = self.classes[class_idx]\n",
        "\n",
        "            if class_name in self.classes_info and self.classes_info[class_name].get(\"type\") == \"rare\":\n",
        "                rare_images.append(img_path)\n",
        "\n",
        "        print(f\"Количество изображений с типом 'rare': {len(rare_images)}\",total)\n",
        "        return rare_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-eBUVMqe1RSw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TestData(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Класс для чтения и хранения тестового датасета.\n",
        "\n",
        "    :param root: путь до папки с картинками знаков\n",
        "    :param path_to_classes_json: путь до classes.json\n",
        "    :param annotations_file: путь до .csv-файла с аннотациями (опциональный)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        path_to_classes_json: str,\n",
        "        annotations_file: str = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "\n",
        "        with open(path_to_classes_json, 'r') as f:\n",
        "            self.classes_info = json.load(f)\n",
        "\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes_info.keys())}\n",
        "\n",
        "        self.samples = []\n",
        "        for img_name in os.listdir(self.root):\n",
        "            img_path = os.path.join(self.root, img_name)\n",
        "            if img_name.endswith(\".png\"):\n",
        "                self.samples.append(img_path)\n",
        "\n",
        "        self.targets = None\n",
        "        if annotations_file is not None:\n",
        "            annotations = pd.read_csv(annotations_file)\n",
        "            self.targets = {row[\"filename\"]: self.class_to_idx[row[\"class\"]] for _, row in annotations.iterrows()}\n",
        "\n",
        "        self.transform = Compose([\n",
        "            Resize(256, 256),\n",
        "            #RandomCrop(224, 224, p=0.5),\n",
        "            #HorizontalFlip(p=0.5),\n",
        "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            Resize(224, 224),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index: int) -> typing.Tuple[torch.Tensor, str, int, str]:\n",
        "        \"\"\"\n",
        "        Возвращает кортеж: тензор с картинкой, путь до файла, номер класса файла (если нет разметки, то \"-1\"),\n",
        "        аннотация (имя класса или \"unknown\" при отсутствии разметки).\n",
        "        \"\"\"\n",
        "        img_path = self.samples[index]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        image = self.transform(image=np.array(image))[\"image\"]\n",
        "\n",
        "        if self.targets is not None:\n",
        "            target = self.targets.get(os.path.basename(img_path), -1)\n",
        "            annotation = (\n",
        "                list(self.class_to_idx.keys())[list(self.class_to_idx.values()).index(target)]\n",
        "                if target != -1\n",
        "                else \"unknown\"\n",
        "            )\n",
        "        else:\n",
        "            target = -1\n",
        "            annotation = \"unknown\"\n",
        "\n",
        "        return image, img_path, target, annotation\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает размер датасета (количество сэмплов).\n",
        "        \"\"\"\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WfYxf2ow5bur"
      },
      "outputs": [],
      "source": [
        "class TestDataRare(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Класс для чтения и хранения тестового датасета, включающего только редкие (rare) классы.\n",
        "\n",
        "    :param root: путь до папки с картинками знаков\n",
        "    :param path_to_classes_json: путь до classes.json\n",
        "    :param annotations_file: путь до .csv-файла с аннотациями (опциональный)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        path_to_classes_json: str,\n",
        "        annotations_file: str = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "\n",
        "        with open(path_to_classes_json, 'r') as f:\n",
        "            self.classes_info = json.load(f)\n",
        "\n",
        "        self.classes_info = {cls: info for cls, info in self.classes_info.items() if info[\"type\"] == \"freq\" }\n",
        "\n",
        "        self.class_to_idx = {cls: info[\"id\"] for cls, info in self.classes_info.items()}\n",
        "\n",
        "        self.samples = []\n",
        "        for img_name in os.listdir(self.root):\n",
        "            img_path = os.path.join(self.root, img_name)\n",
        "            if img_name.endswith(\".png\"):\n",
        "                self.samples.append(img_path)\n",
        "\n",
        "        self.targets = None\n",
        "        if annotations_file is not None:\n",
        "            annotations = pd.read_csv(annotations_file)\n",
        "            self.targets = {\n",
        "                row[\"filename\"]: self.class_to_idx[row[\"class\"]]\n",
        "                for _, row in annotations.iterrows()\n",
        "                if row[\"class\"] in self.class_to_idx\n",
        "            }\n",
        "\n",
        "        self.transform = Compose([\n",
        "            Resize(256, 256),\n",
        "            #RandomCrop(224, 224, p=0.5),\n",
        "            #HorizontalFlip(p=0.5),\n",
        "            Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            Resize(224, 224),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index: int) -> typing.Tuple[torch.Tensor, str, int, str]:\n",
        "        \"\"\"\n",
        "        Возвращает кортеж: тензор с картинкой, путь до файла, номер класса файла (если нет разметки, то \"-1\"),\n",
        "        аннотация (имя класса или \"unknown\" при отсутствии разметки).\n",
        "        \"\"\"\n",
        "        img_path = self.samples[index]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        image = self.transform(image=np.array(image))[\"image\"]\n",
        "\n",
        "        if self.targets is not None:\n",
        "            target = self.targets.get(os.path.basename(img_path), -1)\n",
        "            annotation = (\n",
        "                list(self.class_to_idx.keys())[list(self.class_to_idx.values()).index(target)]\n",
        "                if target != -1\n",
        "                else \"unknown\"\n",
        "            )\n",
        "        else:\n",
        "            target = -1\n",
        "            annotation = \"unknown\"\n",
        "\n",
        "        return image, img_path, target, annotation\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Возвращает размер датасета (количество сэмплов).\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaRnoHRcQ5EN"
      },
      "source": [
        "# Тестирование на редких классах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7R8zvDBQ5EN"
      },
      "outputs": [],
      "source": [
        "weights_path = \"/Users/andreitsyrkunov/Downloads/simple1_model.pth\"\n",
        "\n",
        "state_dict = torch.load(weights_path, map_location=torch.device('mps'))\n",
        "\n",
        "model = CustomNetwork(internal_features=512, num_classes=205)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"Веса успешно загружены!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vkbv1LXDqqy9",
        "outputId": "6742392f-c6ee-43e5-f308-3ae5e6c77f36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomNetwork(\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  )\n",
              "  (relu): ReLU()\n",
              "  (classifier): Linear(in_features=512, out_features=205, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KwSygUWwQ5EO"
      },
      "outputs": [],
      "source": [
        "root = \"/content/additional_files/smalltest\"\n",
        "path_to_classes_json = \"/content/additional_files/classes.json\"\n",
        "annotations_file = \"/content/additional_files/smalltest_annotations.csv\"\n",
        "\n",
        "\n",
        "dataset_rare = TestDataRare(\n",
        "    root=root,\n",
        "    path_to_classes_json=path_to_classes_json,\n",
        "    annotations_file=annotations_file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzqJwHzv6GIt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "root = \"/content/additional_files/smalltest\"\n",
        "path_to_classes_json = \"/content/additional_files/classes.json\"\n",
        "annotations_file = \"/content/additional_files/smalltest_annotations.csv\"\n",
        "\n",
        "\n",
        "dataset_rare = TestDataRare(\n",
        "    root=root,\n",
        "    path_to_classes_json=path_to_classes_json,\n",
        "    annotations_file=annotations_file\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(dataset_rare, batch_size=1, shuffle=False)\n",
        "correct = 0\n",
        "total = 0\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _, labels, x in tqdm(val_loader, desc=\"Validation Progress\", unit=\"batch\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        if labels == -1:\n",
        "          continue\n",
        "        #print(labels)\n",
        "        images = images.float()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        #print(predicted)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "val_accuracy = 100 * correct / total\n",
        "print(f\"Validation Rare Accuracy: {val_accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvALMPP_1UEq"
      },
      "source": [
        "# Модель нейронное сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ykDa8_EK1Rjk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class CustomNetwork(nn.Module):\n",
        "    def __init__(self, num_classes: int, internal_features: int = 512):\n",
        "        super(CustomNetwork, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(in_features, internal_features)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.classifier = nn.Linear(internal_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz-XO97y1hF9"
      },
      "source": [
        "# Trainloop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pPwtSFNU1auS"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "def train_simple_classifier(model, train_dataset, val_dataset, epochs=10, batch_size=32, learning_rate=0.001):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\", leave=False)\n",
        "        for images, _, labels in train_loader_tqdm:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            train_loader_tqdm.set_postfix(loss=running_loss / len(train_loader_tqdm), accuracy=100 * correct / total)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}, Train Accuracy: {train_accuracy}%\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "\n",
        "\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} Validation\", leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, _, labels, __ in val_loader_tqdm:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                val_loader_tqdm.set_postfix(loss=val_loss / len(val_loader_tqdm), accuracy=100 * correct / total)\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}%\")\n",
        "        scheduler.step()\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Learning rate for epoch {epoch+1}: {current_lr}\")\n",
        "    torch.save(model.state_dict(), 'simple2_model.pth')\n",
        "    print(\"Model saved as simple2_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = \"1UJqADXcfy21YbTXujvB7omC2IAnjdDnM\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "output_path = \"/content/FT_model.pth\"\n",
        "\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "model = CustomNetwork(internal_features=512, num_classes=205)\n",
        "#model.load_state_dict(torch.load(output_path))\n",
        "for name, param in model.resnet.named_parameters():\n",
        "    if 'layer4' not in name and 'fc' not in name:\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "OLPxVWd5YzBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wb2dXUsi1mTF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "root_train = \"/content/dataset/synt\"\n",
        "root_val = \"/content/additional_files/smalltest\"\n",
        "classes_json = \"/content/additional_files/classes.json\"\n",
        "annotations_file_val = \"/content/additional_files/smalltest_annotations.csv\"\n",
        "\n",
        "train_dataset = DatasetRTSD(root_folders=[root_train], path_to_classes_json=classes_json)\n",
        "val_dataset = TestData(root=root_val, path_to_classes_json=classes_json, annotations_file=annotations_file_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_simple_classifier(model, train_dataset, val_dataset, epochs=5,batch_size=512)"
      ],
      "metadata": {
        "id": "KL9cReIXVJ7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/FineTuneModelWithSyntRS.pth\"\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "IluSub0ttXE3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ze8FVzb3tbx7",
        "outputId": "081fd84c-c29c-4e9f-ae67-4c6c7f933492"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3f0999f-fc58-4f03-8629-ad424d1a8c58\", \"FineTuneModelWithSyntRS.pth\", 98976014)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ6sDwM81vqo"
      },
      "source": [
        "# Синтез данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "butR8NVJQ5EQ"
      },
      "outputs": [],
      "source": [
        "icons_dir = \"/Users/andreitsyrkunov/Desktop/ML/CV(YSDA)/task 8(road znak)))/additonal_files/icons\"\n",
        "backgrounds_dir = \"/Users/andreitsyrkunov/Desktop/ML/CV(YSDA)/task 8(road znak)))/additonal_files/background_images\"\n",
        "output_dir = \"/Users/andreitsyrkunov/Desktop/ML/CV(YSDA)/task 8(road znak)))/additonal_files/synt\"\n",
        "\n",
        "generator = SignGenerator(icons_dir, backgrounds_dir, output_dir)\n",
        "generator.generate_all_data(num_images_per_class=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgUBcDgtQ5EQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import numpy as np\n",
        "from scipy.ndimage import convolve\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "class SignGenerator:\n",
        "    def __init__(self, icons_dir, backgrounds_dir, output_dir):\n",
        "        self.icons_dir = icons_dir\n",
        "        self.backgrounds_dir = backgrounds_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.background_images = list(Path(backgrounds_dir).glob('*.jpg'))\n",
        "        print(f\"Найдено фонов: {len(self.background_images)}\")\n",
        "        self.icon_files = list(Path(icons_dir).glob('*.png'))\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def resize_icon(self, icon, min_size=16, max_size=128):\n",
        "        size = random.randint(min_size, max_size)\n",
        "        return icon.resize((size, size), Image.Resampling.LANCZOS)\n",
        "\n",
        "    def add_padding(self, icon, max_padding_pct=0.15):\n",
        "        icon_size = icon.size[0]\n",
        "        padding = random.randint(0, int(icon_size * max_padding_pct))\n",
        "        new_size = (icon_size + padding, icon_size + padding)\n",
        "        new_icon = Image.new('RGBA', new_size, (255, 255, 255, 0))\n",
        "        new_icon.paste(icon, (padding // 2, padding // 2))\n",
        "        return new_icon\n",
        "\n",
        "    def rotate_icon(self, icon, min_angle=-15, max_angle=15):\n",
        "        angle = random.randint(min_angle, max_angle)\n",
        "        return icon.rotate(angle, resample=Image.Resampling.BICUBIC, expand=True)\n",
        "\n",
        "    def blur_icon(self, icon, min_angle=-90, max_angle=90):\n",
        "        angle = random.randint(min_angle, max_angle)\n",
        "        kernel = np.ones((3, 3)) / 9.0\n",
        "        icon_array = np.array(icon.convert(\"RGBA\"))\n",
        "        icon_blurred = np.zeros_like(icon_array)\n",
        "        for channel in range(4):\n",
        "            icon_blurred[..., channel] = convolve(icon_array[..., channel], kernel, mode='reflect')\n",
        "        return Image.fromarray(icon_blurred.astype(np.uint8))\n",
        "\n",
        "    def apply_gaussian_blur(self, icon, radius=2):\n",
        "        return icon.filter(ImageFilter.GaussianBlur(radius))\n",
        "\n",
        "    def apply_random_dullness(self, icon):\n",
        "        enhancer = ImageEnhance.Brightness(icon)\n",
        "        dullness_factor = random.uniform(0.5, 0.8)\n",
        "        return enhancer.enhance(dullness_factor)\n",
        "\n",
        "    def embed_on_background(self, icon, background):\n",
        "        icon_size = icon.size[0]\n",
        "        bg_resized = background.resize((icon_size + random.randint(0, int(icon_size * 0.15)),\n",
        "                                        icon_size + random.randint(0, int(icon_size * 0.15))))\n",
        "        bg_resized.paste(icon, (random.randint(0, bg_resized.width - icon_size),\n",
        "                                random.randint(0, bg_resized.height - icon_size)), icon)\n",
        "        return bg_resized\n",
        "\n",
        "    def generate_one_icon(self, icon_class):\n",
        "        icon_files = [file for file in self.icon_files if re.search(f'{icon_class}', file.stem)]\n",
        "        if not icon_files:\n",
        "            raise ValueError(f\"Нет иконок для класса {icon_class} в директории {self.icons_dir}\")\n",
        "\n",
        "        icon_path = random.choice(icon_files)\n",
        "        icon = Image.open(icon_path).convert(\"RGBA\")\n",
        "        icon_resized = self.resize_icon(icon)\n",
        "        icon_with_padding = self.add_padding(icon_resized)\n",
        "        icon_rotated = self.rotate_icon(icon_with_padding)\n",
        "        icon_blurred = self.blur_icon(icon_rotated)\n",
        "        icon_gaussian_blurred = self.apply_gaussian_blur(icon_blurred)\n",
        "        icon_dulled = self.apply_random_dullness(icon_gaussian_blurred)\n",
        "\n",
        "        if not self.background_images:\n",
        "            raise ValueError(\"Нет доступных фонов для встраивания. Проверьте папку с фонами.\")\n",
        "\n",
        "        background = random.choice(self.background_images)\n",
        "        background_image = Image.open(background).convert(\"RGBA\")\n",
        "        final_image = self.embed_on_background(icon_dulled, background_image)\n",
        "        return final_image\n",
        "\n",
        "\n",
        "    def generate_samples_from_image(self, image_path, n=10):\n",
        "        icon = Image.open(image_path).convert(\"RGBA\")\n",
        "\n",
        "\n",
        "        for i in range(n):\n",
        "            if not self.background_images:\n",
        "                raise ValueError(\"Нет доступных фонов в указанной директории.\")\n",
        "            icon_resized = self.resize_icon(icon)\n",
        "            icon_with_padding = self.add_padding(icon_resized)\n",
        "            icon_rotated = self.rotate_icon(icon_with_padding)\n",
        "            icon_blurred = self.blur_icon(icon_rotated)\n",
        "            icon_gaussian_blurred = self.apply_gaussian_blur(icon_blurred)\n",
        "            icon_dulled = self.apply_random_dullness(icon_gaussian_blurred)\n",
        "\n",
        "            background = random.choice(self.background_images)\n",
        "            background_image = Image.open(background).convert(\"RGBA\")\n",
        "\n",
        "            final_image = self.embed_on_background(icon_dulled, background_image)\n",
        "\n",
        "            output_image_path = Path(self.output_dir) / f\"sample_{i}.png\"\n",
        "            final_image.save(output_image_path)\n",
        "\n",
        "        print(f\"Генерация {n} сэмплов завершена, сохранено в {self.output_dir}.\")\n",
        "\n",
        "    def generate_all_data(self, num_images_per_class=1000):\n",
        "        icon_classes = set()\n",
        "        for file in self.icon_files:\n",
        "            class_name = file.stem\n",
        "            icon_classes.add(class_name)\n",
        "\n",
        "        if not icon_classes:\n",
        "            raise ValueError(\"Не удалось извлечь классы из имен файлов.\")\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            for icon_class in icon_classes:\n",
        "                output_class_dir = Path(self.output_dir) / icon_class\n",
        "                output_class_dir.mkdir(parents=True, exist_ok=True)\n",
        "                futures = [executor.submit(self.generate_one_icon, icon_class) for _ in range(num_images_per_class)]\n",
        "                for i, future in enumerate(futures):\n",
        "                    image = future.result()\n",
        "                    image.save(output_class_dir / f\"{i}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHZNtHlSQ5ER"
      },
      "outputs": [],
      "source": [
        "generator = SignGenerator(icons_dir, backgrounds_dir, output_dir)\n",
        "generator.generate_all_data(num_images_per_class=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Метрическое обучение"
      ],
      "metadata": {
        "id": "kKt79qx1SC2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6KnRf0XkQ5ES"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeaturesLoss(nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(FeaturesLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        Вычисляет contrastive loss.\n",
        "\n",
        "        :param features: Tensor, размер (batch_size, feature_dim), векторы признаков объектов.\n",
        "        :param labels: Tensor, размер (batch_size), реальные метки объектов.\n",
        "        :return: Tensor, значение функции потерь.\n",
        "        \"\"\"\n",
        "        batch_size = features.size(0)\n",
        "        loss = 0.0\n",
        "        positive_pairs = 0\n",
        "        negative_pairs = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            for j in range(batch_size):\n",
        "                if i != j:\n",
        "                    distance = torch.norm(features[i] - features[j], p=2) ** 2\n",
        "                    if labels[i] == labels[j]:\n",
        "                        loss += distance\n",
        "                        positive_pairs += 1\n",
        "                    else:\n",
        "                        loss += torch.clamp(self.margin - torch.sqrt(distance), min=0) ** 2\n",
        "                        negative_pairs += 1\n",
        "\n",
        "        if positive_pairs > 0:\n",
        "            loss /= positive_pairs + negative_pairs\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Sampler\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "class CustomBatchSampler(Sampler):\n",
        "    def __init__(self, data_source, elems_per_class, classes_per_batch, use_tqdm=True):\n",
        "        self.data_source = data_source\n",
        "        self.elems_per_class = elems_per_class\n",
        "        self.classes_per_batch = classes_per_batch\n",
        "        self.use_tqdm = use_tqdm\n",
        "\n",
        "        self.class_indices = defaultdict(list)\n",
        "\n",
        "        if self.use_tqdm:\n",
        "            print(\"Building class indices...\")\n",
        "\n",
        "        for idx, (_, _, label) in tqdm(enumerate(data_source), total=len(data_source), disable=not self.use_tqdm, desc=\"Building indices\"):\n",
        "            self.class_indices[label].append(idx)\n",
        "\n",
        "        self.classes = list(self.class_indices.keys())\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_indices = []\n",
        "\n",
        "        for _ in tqdm(range(len(self)), desc=\"Generating batches\", disable=not self.use_tqdm):\n",
        "            selected_classes = random.sample(self.classes, self.classes_per_batch)\n",
        "\n",
        "            batch = []\n",
        "            for c in selected_classes:\n",
        "                class_samples = random.sample(self.class_indices[c], self.elems_per_class)\n",
        "                batch.extend(class_samples)\n",
        "\n",
        "            random.shuffle(batch)\n",
        "            batch_indices.append(batch)\n",
        "\n",
        "            yield torch.tensor(batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source) // (self.elems_per_class * self.classes_per_batch)"
      ],
      "metadata": {
        "id": "EWi0uKRuSq9p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_train = \"/content/mixdataset\"\n",
        "root_val = \"/content/additional_files/smalltest\"\n",
        "classes_json = \"/content/additional_files/classes.json\"\n",
        "annotations_file_val = \"/content/additional_files/smalltest_annotations.csv\"\n",
        "train_dataset = DatasetRTSD(root_folders=[root_train], path_to_classes_json=classes_json)\n",
        "root_val = \"/content/additional_files/smalltest\"\n",
        "val_dataset = TestData(root=root_val, path_to_classes_json=classes_json, annotations_file=annotations_file_val)\n",
        "batch_sampler = CustomBatchSampler(train_dataset, elems_per_class=4, classes_per_batch=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlmmEXO_S02r",
        "outputId": "7ddc6a65-e216-4e7b-bb20-c0b72095063d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building class indices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building indices: 100%|██████████| 284896/284896 [07:46<00:00, 610.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class CustomNetworkMetric(nn.Module):\n",
        "    def __init__(self, num_classes: int, internal_features: int = 512):\n",
        "        super(CustomNetworkMetric, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        in_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(in_features, internal_features)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.classifier = nn.Linear(internal_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        l = self.resnet(x)\n",
        "        x = self.relu(l)\n",
        "        x = self.classifier(x)\n",
        "        return x, l\n"
      ],
      "metadata": {
        "id": "SssR7vj1m9oJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomNetworkMetric(internal_features=512, num_classes=205)\n",
        "import gdown\n",
        "\n",
        "file_id = \"1UJqADXcfy21YbTXujvB7omC2IAnjdDnM\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "output_path = \"/content/FT_model.pth\"\n",
        "\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "model.load_state_dict(torch.load(output_path))\n",
        "\n",
        "for name, param in model.resnet.named_parameters():\n",
        "    if 'layer4' not in name and 'fc' not in name:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "62M28C_lnD7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_better_model(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    sampler,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001,\n",
        "    margin=2.0,\n",
        "    synthetic_weight=0.5,\n",
        "    use_tqdm=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Обучает модель с использованием заданного сэмплера и комбинированной функции потерь.\n",
        "\n",
        "    :param model: torch.nn.Module, модель для обучения.\n",
        "    :param train_dataset: Dataset, тренировочный датасет.\n",
        "    :param val_dataset: Dataset, валидационный датасет.\n",
        "    :param sampler: Sampler, сэмплер для формирования батчей.\n",
        "    :param epochs: int, количество эпох обучения.\n",
        "    :param learning_rate: float, скорость обучения.\n",
        "    :param margin: float, маржа для FeaturesLoss.\n",
        "    :param synthetic_weight: float, вес синтетической части loss.\n",
        "    :param use_tqdm: bool, использовать ли прогресс-бары.\n",
        "    \"\"\"\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "    features_criterion = FeaturesLoss(margin=margin)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_classification_loss = 0.0\n",
        "        running_features_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\", leave=False, disable=not use_tqdm)\n",
        "        for images, _, labels in train_loader_tqdm:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, features = model(images)\n",
        "            classification_loss = classification_criterion(outputs, labels)\n",
        "            features_loss = features_criterion(features, labels)\n",
        "\n",
        "            loss = classification_loss + synthetic_weight * features_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_classification_loss += classification_loss.item()\n",
        "            running_features_loss += features_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            train_loader_tqdm.set_postfix(\n",
        "                loss=running_loss / len(train_loader_tqdm),\n",
        "                classification_loss=running_classification_loss / len(train_loader_tqdm),\n",
        "                features_loss=running_features_loss / len(train_loader_tqdm),\n",
        "                accuracy=100 * correct / total\n",
        "            )\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}, Train Accuracy: {train_accuracy}%\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} Validation\", leave=False, disable=not use_tqdm)\n",
        "        with torch.no_grad():\n",
        "            for images, _, labels in val_loader_tqdm:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs, features = model(images)\n",
        "                classification_loss = classification_criterion(outputs, labels)\n",
        "                features_loss = features_criterion(features, labels)\n",
        "                loss = classification_loss + synthetic_weight * features_loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                val_loader_tqdm.set_postfix(\n",
        "                    loss=val_loss / len(val_loader_tqdm),\n",
        "                    accuracy=100 * correct / total\n",
        "                )\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}%\")\n",
        "        scheduler.step()\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Learning rate for epoch {epoch+1}: {current_lr}\")\n",
        "\n",
        "    torch.save(model.state_dict(), 'better_model.pth')\n",
        "    print(\"Model saved as better_model.pth\")\n"
      ],
      "metadata": {
        "id": "upU-M5g1TS9Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_better_model(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    batch_sampler,\n",
        "    epochs=5,\n",
        "    learning_rate=0.001,\n",
        "    margin=2.0,\n",
        "    synthetic_weight=0.5,\n",
        "    use_tqdm=True\n",
        ")"
      ],
      "metadata": {
        "id": "G3Cvm0Xog4Rw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zJ6sDwM81vqo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}